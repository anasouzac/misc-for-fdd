{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support , roc_auc_score, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from pickle import load\n",
    "from random import randint\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "%run ./base_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixando a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value = randint(0, 99999)\n",
    "print(seed_value)\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prepro(data):\n",
    "    \n",
    "    y_data = data['STATUS'].copy()\n",
    "    x_data = data.drop(['Unnamed: 0', 'STATUS'], 1)\n",
    "    \n",
    "    # Uniformização entre 0 e 1\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    scaled = min_max_scaler.fit_transform(x_data)\n",
    "    x_data_norm = pd.DataFrame(scaled, index=x_data.index, columns=x_data.columns)\n",
    "    \n",
    "    return x_data_norm, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos caminhos das pastas\n",
    "\n",
    "data_folder = \"D:\\\\TEP - Python\\\\\"\n",
    "outputs_folder = \"C:\\\\Users\\\\anaso\\\\Desktop\\\\workspace\\\\doutorado\\\\outputs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = 'RF28_new'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_folder + \"09-python_dados-3anos.csv\", sep=';')\n",
    "data_teste = pd.read_csv(data_folder + \"13-python_dados-1ano.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Treino: \", np.shape(data))\n",
    "print(\"Teste:  \", np.shape(data_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data['STATUS'].copy()\n",
    "x_train = data.drop(['Unnamed: 0', 'STATUS'], 1)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, \n",
    "                                                      test_size=0.15, \n",
    "                                                      random_state=seed_value, \n",
    "                                                      shuffle=False)\n",
    "\n",
    "y_test = data_teste['STATUS'].copy()\n",
    "x_test = data_teste.drop(['Unnamed: 0', 'STATUS'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conferência parcial\n",
    "\n",
    "print(\"TREINO\")\n",
    "print(\"Entradas:      \", np.shape(x_train))\n",
    "print(\"Saída:         \", np.shape(y_train))\n",
    "\n",
    "print(\"\\VALIDAÇÃO\")\n",
    "print(\"Entradas:      \", np.shape(x_valid))\n",
    "print(\"Saída:         \", np.shape(y_valid))\n",
    "\n",
    "print(\"\\nTESTE\")\n",
    "print(\"Entradas:      \", np.shape(x_test))\n",
    "print(\"Saída:         \", np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS = np.sort(y_test.unique())\n",
    "STATUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem do sistema de FDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de class_weight para o caso multilabel\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', \n",
    "                                                  classes=np.unique(y_train), \n",
    "                                                  y=y_train)\n",
    "\n",
    "class_weight_dict = {}\n",
    "for i in range(len(STATUS)):\n",
    "        class_weight_dict[i] = class_weights[i]\n",
    "\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo - Treino simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Definição da topologia do modelo - Random Forest\n",
    "\n",
    "ti = timer()\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               criterion='gini',\n",
    "                               max_depth=50,\n",
    "                               max_features=15,\n",
    "                               bootstrap=True,\n",
    "                               oob_score=True, \n",
    "                               n_jobs=-1,\n",
    "                               random_state=seed_value,\n",
    "                               verbose=2, \n",
    "                               class_weight=class_weight_dict)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "tf = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tempo total: \" + str(int((tf-ti)//60)) + \" minutos e \" + str(math.ceil((tf-ti)%60))+ \" segundos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.oob_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix w/ Heatmap\n",
    "\n",
    "out_train = model.predict(x_train)\n",
    "\n",
    "df_cm_train = pd.DataFrame(confusion_matrix(y_train, out_train), index=[i for i in STATUS], columns=[i for i in STATUS])\n",
    "\n",
    "# Linha para normalizar os dados\n",
    "df_cm_train_norm = round((df_cm_train.astype('float') / df_cm_train.sum(axis=1)[:, np.newaxis]), 2)\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_train_norm, annot=True, cmap='PuBu') # fmt='d'\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = metrics(y_train, out_train, model, df_cm_train, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((train_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((train_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((train_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Accuracy:  {:.2f}%\".format(accuracy_score(y_train, out_train)*100))\n",
    "\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix w/ Heatmap\n",
    "\n",
    "out_valid = model.predict(x_valid)\n",
    "\n",
    "df_cm_valid = pd.DataFrame(confusion_matrix(y_valid, out_valid), index=[i for i in STATUS], columns=[i for i in STATUS])\n",
    "\n",
    "# Linha para normalizar os dados\n",
    "df_cm_valid_norm = round((df_cm_valid.astype('float') / df_cm_valid.sum(axis=1)[:, np.newaxis]), 2)\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_valid_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = metrics(y_valid, out_valid, model, df_cm_valid, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((valid_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((valid_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((valid_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Accuracy:  {:.2f}%\".format(accuracy_score(y_valid, out_valid)*100))\n",
    "\n",
    "valid_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix w/ Heatmap\n",
    "\n",
    "out_test = model.predict(x_test)\n",
    "\n",
    "df_cm_test = pd.DataFrame(confusion_matrix(y_test, out_test), index=[i for i in STATUS], columns=[i for i in STATUS])\n",
    "\n",
    "# Linha para normalizar os dados\n",
    "df_cm_test_norm = round((df_cm_test.astype('float') / df_cm_test.sum(axis=1)[:, np.newaxis]), 2)\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_test_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras métricas do teste - Precision, Recall, F-Score\n",
    "\n",
    "test_metrics = metrics(y_test, out_test, model, df_cm_test, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((test_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((test_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((test_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Accuracy:  {:.2f}%\".format(accuracy_score(y_test, out_test)*100))\n",
    "\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construção da curva ROC para o caso binário (base: classe 0)\n",
    "\n",
    "# Dados reais em OHE\n",
    "y_test = pd.DataFrame(y_test).astype('category')\n",
    "y_test = pd.get_dummies(y_test).values\n",
    "\n",
    "# Predições em OHE\n",
    "y_pred = pd.DataFrame(out_test).astype('category')\n",
    "y_pred = pd.get_dummies(y_pred).values\n",
    "\n",
    "n_classes = len(STATUS)\n",
    "\n",
    "# Calcula a curva ROC e a métrica AUC para cada classe\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i], )\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange', lw=2, label='Curva ROC (area = %0.3f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de falsos positivos')\n",
    "plt.ylabel('Taxa de verdadeiros positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC e cálculo da métrica AUC para todas as classes\n",
    "\n",
    "roc_auc_scores = []\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12,12), dpi=300)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.3f})'''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:0.3f})'''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.3f})'''.format(i, roc_auc[i]))\n",
    "    roc_auc_scores.append(roc_auc[i])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de falsos positivos')\n",
    "plt.ylabel('Taxa de verdadeiros positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, outputs_folder + \"random_forest-\" + TEST_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_rf = joblib.load(outputs_folder + \"random_forest-\" + TEST_CODE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
