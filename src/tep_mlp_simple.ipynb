{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support , roc_auc_score, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import interp\n",
    "from pickle import load\n",
    "from random import randint\n",
    "from timeit import default_timer as timer\n",
    "from itertools import cycle\n",
    "\n",
    "%run ./base_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixando a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value = randint(0, 99999)\n",
    "print(seed_value)\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prepro(data):\n",
    "    \n",
    "    y_data = data['STATUS'].copy()\n",
    "    x_data = data.drop(['Unnamed: 0', 'STATUS'], 1)\n",
    "    \n",
    "    # Data standardization\n",
    "    scaled = preprocessing.scale(x_data)\n",
    "    x_data_norm = pd.DataFrame(scaled, index=x_data.index, columns=x_data.columns)\n",
    "    \n",
    "    y_df = pd.DataFrame(y_data).astype('category')\n",
    "    y_df_ohe = pd.get_dummies(y_df)\n",
    "    y_ohe = y_df_ohe.values\n",
    "    \n",
    "    return x_data_norm, y_data, y_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos caminhos das pastas\n",
    "\n",
    "data_folder = \"D:\\\\4. Doutorado\\\\Tese\\\\3. Estudos de caso\\\\Caso 1 - Tennessee Eastman\\\\4. Bancos simulados - Python\\\\\"\n",
    "outputs_folder = \"C:\\\\Users\\\\anaso\\\\Desktop\\\\workspace\\\\doutorado\\\\outputs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = 'MLP1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_folder + \"09-python_dados-3anos.csv\", sep=';')\n",
    "x_data = data.drop(['Unnamed: 0', 'STATUS'], 1)\n",
    "y_data = data['STATUS'].copy()\n",
    "\n",
    "data_teste = pd.read_csv(data_folder + \"13-python_dados-1ano.csv\", sep=';')\n",
    "x_data_teste = data_teste.drop(['Unnamed: 0', 'STATUS'], 1)\n",
    "y_test_multi = data_teste['STATUS'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Treino: \", np.shape(data))\n",
    "print(\"Teste:  \", np.shape(data_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em treino e validação\n",
    "\n",
    "X_train, X_valid, y_train_multi, y_valid_multi = train_test_split(x_data, y_data, \n",
    "                                                                  test_size=0.15, \n",
    "                                                                  random_state=seed_value, \n",
    "                                                                  shuffle=False)\n",
    "\n",
    "print(\"\\nTREINO\")\n",
    "print(\"X: \", np.shape(X_train))\n",
    "print(\"Y: \", np.shape(y_train_multi))\n",
    "print(\"Status:\", Counter(y_train_multi))\n",
    "\n",
    "print(\"\\nVALIDAÇÃO\")\n",
    "print(\"X: \", np.shape(X_valid))\n",
    "print(\"Y: \", np.shape(y_valid_multi))\n",
    "print(\"Status:\", Counter(y_valid_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS = np.unique(y_train_multi)\n",
    "STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "x_train = scaler.transform(X_train)\n",
    "x_valid = scaler.transform(X_valid)\n",
    "x_test = scaler.transform(x_data_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train_multi, num_classes=len(STATUS))\n",
    "y_valid = to_categorical(y_valid_multi, num_classes=len(STATUS))\n",
    "y_test = to_categorical(y_test_multi, num_classes=len(STATUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conferência final\n",
    "\n",
    "print(\"TREINO\")\n",
    "print(\"Entradas:      \", np.shape(x_train))\n",
    "print(\"Saída:         \", np.shape(y_train))\n",
    "\n",
    "print(\"\\VALIDAÇÃO\")\n",
    "print(\"Entradas:      \", np.shape(x_valid))\n",
    "print(\"Saída:         \", np.shape(y_valid))\n",
    "\n",
    "print(\"\\nTESTE\")\n",
    "print(\"Entradas:      \", np.shape(x_test))\n",
    "print(\"Saída:         \", np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem do sistema de FDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de class_weight para o caso multilabel\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', \n",
    "                                                  classes=STATUS, \n",
    "                                                  y=y_train.argmax(axis=1))\n",
    "\n",
    "class_weight_dict = {}\n",
    "for i in range(len(STATUS)):\n",
    "        class_weight_dict[i] = class_weights[i]\n",
    "\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo - Treino simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(40, input_dim=x_train.shape[1], activation ='relu')) \n",
    "model.add(Dense(30, activation ='relu')) \n",
    "model.add(Dense(len(STATUS), activation ='softmax')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='auto', patience=20, restore_best_weights=True)\n",
    "model_check = ModelCheckpoint(filepath=outputs_folder+\"model_cp_\"+TEST_CODE+\".h5\", monitor=\"val_loss\", mode=\"auto\")\n",
    "\n",
    "ti = timer()\n",
    "\n",
    "# Treinamento do modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), \n",
    "              metrics=['acc', Precision(), Recall()])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=500, \n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    verbose=1,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    callbacks=[early_stopping, model_check]) \n",
    "\n",
    "save_model(model=model, iterator=TEST_CODE, train_type='simple', model_name='MLP')\n",
    "\n",
    "tf = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tempo total: \" + str(int((tf-ti)//60)) + \" minutos e \" + str(math.ceil((tf-ti)%60))+ \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos - Treinamento x Validação\n",
    "\n",
    "# Informações do treinamento\n",
    "try:\n",
    "    train_acc = history.history[model.metrics_names[1]]\n",
    "    train_loss = history.history[model.metrics_names[0]]\n",
    "    train_precision = history.history[model.metrics_names[2]]\n",
    "    train_recall = history.history[model.metrics_names[3]]\n",
    "except:\n",
    "    # quando o history vem do CSV salvo do melhor modelo\n",
    "    train_acc = history['acc']\n",
    "    train_loss = history['loss']\n",
    "    train_precision = history['precision_'+str(best_trial)]\n",
    "    train_recall = history['recall_'+str(best_trial)]\n",
    "\n",
    "# Informações da validação\n",
    "try:\n",
    "    val_acc = history.history['val_'+str(model.metrics_names[1])]\n",
    "    val_loss = history.history['val_'+str(model.metrics_names[0])]\n",
    "    val_precision = history.history['val_'+str(model.metrics_names[2])]\n",
    "    val_recall = history.history['val_'+str(model.metrics_names[3])]\n",
    "except:\n",
    "    val_acc = history['val_acc']\n",
    "    val_loss = history['val_loss']\n",
    "    val_precision = history['val_precision_'+str(best_trial)]\n",
    "    val_recall = history['val_recall_'+str(best_trial)]\n",
    "\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "print(\"Épocas: \", len(epochs))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_acc, '-bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, '-ko', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_loss, '-bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, '-ko', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_precision, '-bo', label='Training precision')\n",
    "plt.plot(epochs, val_precision, '-ko', label='Validation precision')\n",
    "plt.title('Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, train_recall, '-bo', label='Training recall')\n",
    "plt.plot(epochs, val_recall, '-ko', label='Validation recall')\n",
    "plt.title('Recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREINAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix w/ Heatmap\n",
    "\n",
    "out_train = model.predict(x_train)\n",
    "\n",
    "df_cm_train = pd.DataFrame(confusion_matrix(y_train.argmax(axis=1), out_train.argmax(axis=1)), \\\n",
    "                           index = [i for i in STATUS], columns = [i for i in STATUS])\n",
    "\n",
    "# Linha para normalizar os dados\n",
    "df_cm_train_norm = round((df_cm_train.astype('float') / df_cm_train.sum(axis=1)[:, np.newaxis]), 2)\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_train_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras métricas do treinamento - Precision, Recall, F-Score\n",
    "\n",
    "train_metrics = metrics(y_train, out_train, model, df_cm_train, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((train_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((train_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((train_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall AUC:       {:.2f}%\".format((roc_auc_score(y_train, out_train)*100)))\n",
    "print(\"Overall Accuracy:  {:.2f}%\".format(accuracy_score(y_train.argmax(axis=1), out_train.argmax(axis=1))*100))\n",
    "\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_valid = model.predict(x_valid)\n",
    "\n",
    "df_cm_valid = pd.DataFrame(confusion_matrix(y_valid.argmax(axis=1), out_valid.argmax(axis=1)), \\\n",
    "                           index = [i for i in STATUS], columns = [i for i in STATUS])\n",
    "\n",
    "# Linha para normalizar os dados\n",
    "df_cm_valid_norm = round((df_cm_valid.astype('float') / df_cm_valid.sum(axis=1)[:, np.newaxis]), 2)\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_valid_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras métricas do treinamento - Precision, Recall, F-Score\n",
    "\n",
    "valid_metrics = metrics(y_valid, out_valid, model, df_cm_valid, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((valid_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((valid_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((valid_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall AUC:       {:.2f}%\".format((roc_auc_score(y_valid, out_valid)*100)))\n",
    "print(\"Overall Accuracy:  {:.2f}%\".format(accuracy_score(y_valid.argmax(axis=1), out_valid.argmax(axis=1))*100))\n",
    "\n",
    "valid_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = model.predict(x_test)\n",
    "\n",
    "df_cm_test = pd.DataFrame(confusion_matrix(y_test.argmax(axis=1), out_test.argmax(axis=1)), \\\n",
    "                          index = [i for i in STATUS], columns = [i for i in STATUS])\n",
    "\n",
    "# Linha para normalizar os dados\n",
    "df_cm_test_norm = round((df_cm_test.astype('float') / df_cm_test.sum(axis=1)[:, np.newaxis]), 2)\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_test_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras métricas do teste - Precision, Recall, F-Score\n",
    "\n",
    "test_metrics = metrics(y_test, out_test, model, df_cm_test, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((test_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((test_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((test_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall AUC:       {:.2f}%\".format((roc_auc_score(y_test, out_test)*100)))\n",
    "print(\"Overall Accuracy:  {:.2f}%\".format(accuracy_score(y_test.argmax(axis=1), out_test.argmax(axis=1))*100))\n",
    "\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construção da curva ROC para o caso binário (base: classe 0)\n",
    "\n",
    "# Dados reais em OHE\n",
    "# y_test = pd.DataFrame(y_test).astype('category')\n",
    "# y_test = pd.get_dummies(y_test).values\n",
    "\n",
    "# Predições em OHE\n",
    "y_pred = out_test.copy()\n",
    "\n",
    "n_classes = len(STATUS)\n",
    "\n",
    "# Calcula a curva ROC e a métrica AUC para cada classe\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i], )\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange', lw=2, label='Curva ROC (area = %0.3f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de falsos positivos')\n",
    "plt.ylabel('Taxa de verdadeiros positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC e cálculo da métrica AUC para todas as classes\n",
    "\n",
    "roc_auc_scores = []\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12,12), dpi=300)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.3f})'''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:0.3f})'''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.3f})'''.format(i, roc_auc[i]))\n",
    "    roc_auc_scores.append(roc_auc[i])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de falsos positivos')\n",
    "plt.ylabel('Taxa de verdadeiros positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
