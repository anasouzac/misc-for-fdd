{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from timeit import default_timer as timer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support , roc_auc_score, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from pickle import load\n",
    "from random import randint\n",
    "\n",
    "%run ./base_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixando a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value = randint(0, 99999)\n",
    "print(seed_value)\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prepro(data):\n",
    "    \n",
    "    y_data = data['STATUS'].copy()\n",
    "    x_data = data.drop(['Unnamed: 0', 'STATUS'], 1)\n",
    "    \n",
    "    # Data standardization\n",
    "    scaled = preprocessing.scale(x_data)\n",
    "    x_data_norm = pd.DataFrame(scaled, index=x_data.index, columns=x_data.columns)\n",
    "    \n",
    "    y_df = pd.DataFrame(y_data).astype('category')\n",
    "    y_df_ohe = pd.get_dummies(y_df)\n",
    "    y_ohe = y_df_ohe.values\n",
    "    \n",
    "    return x_data_norm, y_data, y_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos caminhos das pastas\n",
    "\n",
    "data_folder = \"D:\\\\TEP - Python\\\\\"\n",
    "outputs_folder = \"C:\\\\Users\\\\anaso\\\\Desktop\\\\workspace\\\\doutorado\\\\outputs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = 'SVM1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_folder + \"09-python_dados-3anos.csv\", sep=';')\n",
    "data_teste = pd.read_csv(data_folder + \"13-python_dados-1ano.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Treino: \", np.shape(data))\n",
    "print(\"Teste:  \", np.shape(data_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, y_train_ohe = df_prepro(data)\n",
    "x_test, y_test, y_test_ohe = df_prepro(data_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, \n",
    "                                                      test_size=0.15, \n",
    "                                                      random_state=seed_value, \n",
    "                                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TREINO\")\n",
    "print(\"Entradas: \", np.shape(x_train))\n",
    "print(\"Saída:    \", np.shape(y_train))\n",
    "\n",
    "print(\"\\nVALIDAÇÃO\")\n",
    "print(\"Entradas: \", np.shape(x_valid))\n",
    "print(\"Saída:    \", np.shape(y_valid))\n",
    "\n",
    "print(\"\\nTESTE\")\n",
    "print(\"Entradas: \", np.shape(x_test))\n",
    "print(\"Saída:    \", np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS = np.sort(y_test.unique())\n",
    "STATUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem do sistema de FDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de class_weight para o caso multilabel\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', \n",
    "                                                  classes=np.unique(y_train), \n",
    "                                                  y=y_train)\n",
    "\n",
    "class_weight_dict = {}\n",
    "for i in range(len(STATUS)):\n",
    "        class_weight_dict[i] = class_weights[i]\n",
    "\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação inicial (treino simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Definição da topologia do modelo - Suport Vector Machines\n",
    "\n",
    "model = SVC(kernel='rbf', verbose=1, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Treinamento da rede convolucional\n",
    "\n",
    "ti = timer()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "tf = timer()\n",
    "\n",
    "# Salvando o modelo\n",
    "pickle.dump(model, open(outputs_folder + \"tep_svm-simple-\" + TEST_CODE, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tempo total: \" + str(int((tf-ti)//60)) + \" minutos e \" + str(math.ceil((tf-ti)%60))+ \" segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREINAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix w/ Heatmap\n",
    "\n",
    "out_train = model.predict(x_train)\n",
    "\n",
    "df_cm_train = pd.DataFrame(confusion_matrix(y_train, out_train), index = [i for i in STATUS],\n",
    "                  columns = [i for i in STATUS])\n",
    "\n",
    "# Normalizar os dados (tive que fazer manualmente)\n",
    "df_cm_train_norm = pd.DataFrame(np.zeros((21, 21)))\n",
    "for row in range(len(STATUS)):\n",
    "    \n",
    "    denominador = df_cm_train.sum(axis=1)[:, np.newaxis][row].item()\n",
    "    \n",
    "    for col in range(len(STATUS)):\n",
    "        \n",
    "        numerador = df_cm_train.iloc[row,col].astype('float')\n",
    "        divisao = round((numerador/denominador), 2)\n",
    "        df_cm_train_norm.iloc[row,col] = divisao\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_train_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras métricas do treinamento - Precision, Recall, F-Score\n",
    "\n",
    "train_metrics = metrics(y_train, out_train, model, df_cm_train, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((train_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((train_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((train_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix w/ Heatmap\n",
    "\n",
    "out_valid = model.predict(x_valid)\n",
    "\n",
    "df_cm_valid = pd.DataFrame(confusion_matrix(y_valid, out_valid), index = [i for i in STATUS],\n",
    "                  columns = [i for i in STATUS])\n",
    "\n",
    "# Normalizar os dados (tive que fazer manualmente)\n",
    "df_cm_valid_norm = pd.DataFrame(np.zeros((21, 21)))\n",
    "for row in range(len(STATUS)):\n",
    "    \n",
    "    denominador = df_cm_valid.sum(axis=1)[:, np.newaxis][row].item()\n",
    "    \n",
    "    for col in range(len(STATUS)):\n",
    "        \n",
    "        numerador = df_cm_valid.iloc[row,col].astype('float')\n",
    "        divisao = round((numerador/denominador), 2)\n",
    "        df_cm_valid_norm.iloc[row,col] = divisao\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_valid_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras métricas da validação - Precision, Recall, F-Score\n",
    "\n",
    "train_metrics = metrics(y_valid, out_valid, model, df_cm_valid, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((valid_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((valid_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((valid_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "\n",
    "valid_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = model.predict(x_test)\n",
    "\n",
    "df_cm_test = pd.DataFrame(confusion_matrix(y_test, out_test), index = [i for i in STATUS],\n",
    "                  columns = [i for i in STATUS])\n",
    "\n",
    "# Normalizar os dados (tive que fazer manualmente)\n",
    "df_cm_test_norm = pd.DataFrame(np.zeros((21, 21)))\n",
    "for row in range(len(STATUS)):\n",
    "    \n",
    "    denominador = df_cm_test.sum(axis=1)[:, np.newaxis][row].item()\n",
    "    \n",
    "    for col in range(len(STATUS)):\n",
    "        \n",
    "        numerador = df_cm_test.iloc[row,col].astype('float')\n",
    "        divisao = round((numerador/denominador), 2)\n",
    "        df_cm_test_norm.iloc[row,col] = divisao\n",
    "\n",
    "plt.figure(figsize = (13,12), dpi=600)\n",
    "ax = sn.heatmap(df_cm_test_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"CLASSES PREDITAS\", fontsize=12)\n",
    "ax.set_ylabel(\"CLASSES REAIS\", fontsize=12)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outras métricas do teste - Precision, Recall, F-Score\n",
    "\n",
    "test_metrics = metrics(y_test, out_test, model, df_cm_test, STATUS, multi_problem=True)\n",
    "\n",
    "print(\"\\nOverall Precision: {:.2f}%\".format((test_metrics['Precision'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall Recall:    {:.2f}%\".format((test_metrics['Recall'].sum()/len(STATUS)*100)))\n",
    "print(\"Overall F1-Score:  {:.2f}%\".format((test_metrics['F-score(a=1)'].sum()/len(STATUS)*100)))\n",
    "\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construção da curva ROC para o caso binário (base: classe 0)\n",
    "\n",
    "# Dados reais em OHE\n",
    "y_test = pd.DataFrame(y_test).astype('category')\n",
    "y_test = pd.get_dummies(y_test).values\n",
    "\n",
    "# Predições em OHE\n",
    "y_pred = pd.DataFrame(out_test).astype('category')\n",
    "y_pred = pd.get_dummies(y_pred).values\n",
    "\n",
    "n_classes = len(STATUS)\n",
    "\n",
    "# Calcula a curva ROC e a métrica AUC para cada classe\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i], )\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange', lw=2, label='Curva ROC (area = %0.3f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de falsos positivos')\n",
    "plt.ylabel('Taxa de verdadeiros positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC e cálculo da métrica AUC para todas as classes\n",
    "\n",
    "roc_auc_scores = []\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12,12), dpi=300)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.3f})'''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:0.3f})'''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.3f})'''.format(i, roc_auc[i]))\n",
    "    roc_auc_scores.append(roc_auc[i])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de falsos positivos')\n",
    "plt.ylabel('Taxa de verdadeiros positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk\n",
    "# model = pickle.load(open(PATH+\"tep_svm-simple-\"+TEST_CODE, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
