{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5c663e",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8dd9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support , roc_auc_score, auc, precision_score\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "%run ./base_functions.ipynb\n",
    "\n",
    "pd.options.mode.chained_assignment = None # desabilita warnings de SettingWithCopyWarning\n",
    "plt.rcParams.update({'figure.max_open_warning': 0}) # desabilita warning para plot de mais de 20 figuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho no qual as imagens geradas serão salvas quando exportadas\n",
    "saving_path = r\"C:\\Users\\anaso\\Desktop\\workspace\\doutorado\\imagens\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762949d",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA():\n",
    "   \n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Método construtor\n",
    "    # Função que será chamada toda vez que um objeto PCA for inicializado\n",
    "    # O modelo selecionará quantos PCs forem necessários para cumprir o % de variabilidade explicada especificado em a\n",
    "\n",
    "    def __init__ (self, a=0.9):\n",
    "\n",
    "        # se 0<=a<1,  'a' indica a fraçao de variancia explicada desejada\n",
    "        # se a>=1,    'a' indica o numero de componentes desejado\n",
    "        self.a = a\n",
    "   \n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Função para treino do modelo\n",
    "    # X são os dados de treino\n",
    "    \n",
    "    def fit(self, X, conf_Q=0.99, conf_T2=0.99, plot=True):\n",
    "    \n",
    "        # guardando médias e desvios-padrão do treino (de cada coluna)\n",
    "        self.mu_train = X.mean(axis=0)\n",
    "        self.std_train = X.std(axis=0)        \n",
    "       \n",
    "        # normalizando dados de treino\n",
    "        X = np.array(((X - self.mu_train)/self.std_train))\n",
    "       \n",
    "        # calculando a matriz de covariâncias dos dados\n",
    "        Cx = np.cov(X, rowvar=False)\n",
    "        \n",
    "        # aplicando decomposição em autovalores e autovetores\n",
    "        self.L, self.P = np.linalg.eig(Cx)\n",
    "        \n",
    "        # frações da variância explicada\n",
    "        fv = self.L/np.sum(self.L)\n",
    "        \n",
    "        # frações da variância explicada acumuladas\n",
    "        fva = np.cumsum(self.L)/sum(self.L)\n",
    "       \n",
    "        # definindo número de componentes\n",
    "        if (self.a>0 and self.a<1):\n",
    "            self.a = np.where(fva>self.a)[0][0]+1 \n",
    "            \n",
    "        # calculando limites de detecção\n",
    "\n",
    "        # limite da estatística T^2\n",
    "        from scipy.stats import f\n",
    "        F = f.ppf(conf_T2, self.a, X.shape[0]-self.a)\n",
    "        self.T2_lim = ((self.a*(X.shape[0]**2-1))/(X.shape[0]*(X.shape[0]-self.a)))*F\n",
    "        print(\"T2_lim: \", self.T2_lim)\n",
    "        \n",
    "        # limite da estatística Q\n",
    "        theta = [np.sum(self.L[self.a:]**(i)) for i in (1,2,3)]\n",
    "        ho = 1-((2*theta[0]*theta[2])/(3*(theta[1]**2)))\n",
    "        from scipy.stats import norm\n",
    "        nalpha = norm.ppf(conf_Q)\n",
    "        self.Q_lim = (theta[0]*(((nalpha*np.sqrt(2*theta[1]*ho**2))/theta[0])+1+\n",
    "                                ((theta[1]*ho*(ho-1))/theta[0]**2))**(1/ho))\n",
    "        print(\"Q_lim: \", self.Q_lim)\n",
    "        \n",
    "        # plotando variâncias explicadas\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.bar(np.arange(len(fv)),fv)\n",
    "            ax.plot(np.arange(len(fv)),fva)\n",
    "            ax.set_xlabel('Número de componentes')\n",
    "            ax.set_ylabel('Variância dos dados')\n",
    "            ax.set_title('PCA - Variância Explicada');\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Função para teste do modelo\n",
    "            \n",
    "    def predict(self, X):\n",
    "            \n",
    "        # normalizando dados de teste (usando os parâmetros do treino!)\n",
    "        X = np.array((X - self.mu_train)/self.std_train)\n",
    "\n",
    "        # calculando estatística T^2\n",
    "        T = X@self.P[:,:self.a]\n",
    "        self.T2 = np.array([T[i,:]@np.linalg.inv(np.diag(self.L[:self.a]))@T[i,:].T for i in range(X.shape[0])])\n",
    "        \n",
    "        # calculando estatística Q\n",
    "        e = X - X@self.P[:,:self.a]@self.P[:,:self.a].T\n",
    "        self.Q  = np.array([e[i,:]@e[i,:].T for i in range(X.shape[0])])\n",
    "        \n",
    "        # calculando contribuições para Q\n",
    "        self.c = np.absolute(X*e) \n",
    "                \n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Função para plotar cartas de controle\n",
    "    \n",
    "    def plot_control_charts(self, fault = None):\n",
    "     \n",
    "        fig, ax = plt.subplots(1,2, figsize=(15,3))\n",
    "        \n",
    "        ax[0].semilogy(self.T2,'.')\n",
    "        ax[0].axhline(self.T2_lim,ls='--',c='r');\n",
    "        ax[0].set_title('Carta de Controle $T^2$')\n",
    "        \n",
    "        ax[1].semilogy(self.Q,'.')\n",
    "        ax[1].axhline(self.Q_lim,ls='--',c='r')\n",
    "        ax[1].set_title('Carta de Controle Q')\n",
    " \n",
    "        if fault is not None:\n",
    "            ax[0].axvline(fault, c='w')\n",
    "            ax[1].axvline(fault, c='w')\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Função para plotar mapas de contribuição\n",
    "            \n",
    "    def plot_contributions(self, fault=None, index=None, columns=None):\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "        \n",
    "        c = pd.DataFrame(self.c, index=index, columns=columns)\n",
    "    \n",
    "        sns.heatmap(c, ax=ax, yticklabels=int(self.c.shape[0]/10), cmap = plt.cm.Blues)\n",
    "        \n",
    "        ax.set_title('Contribuições parciais para Q')\n",
    "        \n",
    "        if fault is not None:\n",
    "            ax.axhline(y=c.index[fault], ls='--', c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_overview(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retorna algumas informações gerais sobre o banco de dados, como existência de nulos e duplicatas, dentre outros.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shape do banco\n",
    "    print(\"Shape do banco: \", data.shape)\n",
    "    \n",
    "    # Alguma linha com todos os valores zerados?\n",
    "    zero_rows = data.index[(data==0).all(axis=1)].tolist()\n",
    "    print(\"\\nQuantidade de linhas zeradas: \", len(zero_rows))\n",
    "    \n",
    "    # Existem nulos?\n",
    "    print(\"Quantidade de nulos no banco: \", data.isnull().sum().sum())\n",
    "    \n",
    "    # Há duplicatas baseado no timestamp?\n",
    "    qtd_duplis = len(data[data.duplicated(['timestamp'])])\n",
    "    if (qtd_duplis > 0):\n",
    "        print(\"Quantidade de duplicatas: \", qtd_duplis)\n",
    "#         print(data[data.duplicated(['timestamp'])])\n",
    "    else:\n",
    "        print(\"\\nNão há registros duplicados.\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Alguma coluna possui um único valor constante?\n",
    "    single_values_columns = []\n",
    "    for col in (data.columns):\n",
    "        unique_values = data[col].nunique()\n",
    "        if (unique_values == 1):\n",
    "            single_values_columns.append(col)\n",
    "    if len(single_values_columns) > 0:\n",
    "        print(\"Existem \" + str(len(single_values_columns)) + \" colunas constantes. São elas: \" + str(single_values_columns))\n",
    "    else:\n",
    "        print(\"Não há colunas constantes.\")\n",
    "    \n",
    "    # Alguma linha inteiramente zerada?\n",
    "    data_sem_dt = data.drop(['timestamp'], 1)\n",
    "    zero_rows = data_sem_dt.index[(data_sem_dt == 0).all(axis=1)].tolist()\n",
    "    if len(zero_rows) > 0:\n",
    "        print(\"Existem \" + str(len(zero_rows)) + \" linhas inteiramente zeradas.\")\n",
    "    else:\n",
    "        print(\"Não há linhas inteiramente zeradas.\")\n",
    "    \n",
    "    # Alguma linha com todas as colunas NaN?\n",
    "    nan_rows = data.index[(data == float('nan')).all(axis=1)].tolist()\n",
    "    if len(nan_rows) > 0:\n",
    "        print(\"Existem \" + str(len(nan_rows)) + \" instâncias sem registro.\")\n",
    "    else:\n",
    "        print(\"Não há linhas inteiramente sem registro (nan).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b24eac",
   "metadata": {},
   "source": [
    "# Leitura dos arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd021b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"D:\\Caso 2 - Bomba Centrífuga\\final_data.csv\", sep=';')\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_log = pd.read_excel(\"D:\\Caso 2 - Bomba Centrífuga\\Bancos tratados\\Falhas_Bomba_final_v0.xlsx\")\n",
    "\n",
    "fault_log['InicioAvar'] = pd.to_datetime(fault_log['InicioAvar'])\n",
    "fault_log['Dt.real fim'] = pd.to_datetime(fault_log['Dt.real fim'])\n",
    "\n",
    "# Remoção das falhas não marcadas - longas ou contidas em outras falhas\n",
    "fault_log = fault_log.dropna(subset=['Filtro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7535158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações gerais sobre o banco\n",
    "\n",
    "print_overview(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead55a2",
   "metadata": {},
   "source": [
    "# Banco 1 - Rótulos via OMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13730e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = data.copy()\n",
    "\n",
    "labeled_data['rotulos_multi'] = np.zeros(())\n",
    "labeled_data['rotulos_bin'] = np.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fault_log)):\n",
    "    \n",
    "    ID = fault_log.iloc[i]['ID']\n",
    "    \n",
    "    print(\"\\n--------------------------------------------------------------------------------\")\n",
    "    print(\"Falha \", (ID))\n",
    "\n",
    "    init_falha = fault_log.iloc[i]['InicioAvar']\n",
    "    init_falha = datetime.datetime.combine(init_falha, datetime.time.min)\n",
    "\n",
    "    end_falha = fault_log.iloc[i]['Dt.real fim']\n",
    "    end_falha = datetime.datetime.combine(end_falha, datetime.time.max)\n",
    "\n",
    "    df_fault_final = labeled_data.loc[((labeled_data[\"timestamp\"] >= init_falha) & \n",
    "                                       (labeled_data[\"timestamp\"] <= end_falha))]\n",
    "    elapsed_time = end_falha - init_falha\n",
    "\n",
    "    print(\"Dias:  \", elapsed_time.days)\n",
    "    print(\"Horas: \", divmod(elapsed_time.total_seconds(), 3600)[0])\n",
    "    print(\"Instâncias: \", len(df_fault_final))\n",
    "\n",
    "    if (len(df_fault_final) == 0):\n",
    "        print(\"Falha \" + str(ID) + \" não existe!\")\n",
    "    else:\n",
    "        labeled_data.loc[\n",
    "            ((labeled_data[\"timestamp\"] >= init_falha) & (labeled_data[\"timestamp\"] <= end_falha)), 'rotulos_multi'] = ID\n",
    "\n",
    "        labeled_data.loc[\n",
    "            ((labeled_data[\"timestamp\"] >= init_falha) & (labeled_data[\"timestamp\"] <= end_falha)), 'rotulos_bin'] = 1\n",
    "\n",
    "    print(\"Init falha: \", labeled_data.loc[labeled_data['rotulos_multi'] == ID]['timestamp'].min())\n",
    "    print(\"End falha : \", labeled_data.loc[labeled_data['rotulos_multi'] == ID]['timestamp'].max())\n",
    "    print(\"\\nReal:\")\n",
    "    print(\"Init falha: \", init_falha)\n",
    "    print(\"End falha : \", end_falha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663fe06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Counter(labeled_data['rotulos_multi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eee8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(labeled_data['rotulos_bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357865a8",
   "metadata": {},
   "source": [
    "# Divisão dos bancos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b33f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(data_folder + \"banco_labeling_params-v0.csv\", sep=';')[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334989d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_treino = labeled_data.loc[:1212560,:]\n",
    "x_treino = x_treino[x_treino['rotulos_multi'] == 0].copy()\n",
    "y_treino = x_treino[['rotulos_multi']].copy()\n",
    "x_treino.drop(['rotulos_multi', 'rotulos_bin', 'timestamp'], axis=1, inplace=True)\n",
    "\n",
    "x_test = labeled_data.loc[1212560:,:]\n",
    "y_test = labeled_data[['rotulos_multi']].loc[1212560:]\n",
    "\n",
    "print(\"TREINO:\")\n",
    "print(x_treino.shape)\n",
    "print(y_treino.shape)\n",
    "print(Counter(y_treino['rotulos_multi']))\n",
    "print()\n",
    "\n",
    "print(\"\\nTESTE:\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(Counter(y_test['rotulos_multi']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029128d9",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdbf08",
   "metadata": {},
   "source": [
    "#### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o objeto PCA\n",
    "pca = PCA(a=0.9)\n",
    "\n",
    "# Treina o modelo PCA\n",
    "pca.fit(x_treino)\n",
    "\n",
    "print(\"\\nPCs: \", pca.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f9d3d",
   "metadata": {},
   "source": [
    "#### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = x_test.drop(['timestamp', 'rotulos_multi', 'rotulos_bin'], axis=1)\n",
    "pca.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nT2: {(pca.T2>pca.T2_lim).sum()/pca.T2.shape[0]}')\n",
    "print(f'Q:  {(pca.Q>pca.Q_lim).sum()/pca.Q.shape[0]}')\n",
    "\n",
    "pca.plot_control_charts()\n",
    "plt.suptitle(f'Banco de teste');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results = pd.DataFrame(columns=['T2', 'Q'])\n",
    "pca_results['T2'] = pca.T2\n",
    "pca_results['Q'] = pca.Q \n",
    "pca_results['rotulos_multi'] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fb433",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results['T2_classific'] = np.where(pca_results['T2'] > pca.T2_lim, 1, 0)\n",
    "pca_results['Q_classific'] = np.where(pca_results['Q'] > pca.Q_lim, 1, 0)\n",
    "pca_results['rotulos_bin'] = np.where(pca_results['rotulos_multi'] >= 1, 1, 0)\n",
    "pca_results = pca_results.reset_index().drop(['index'], axis=1)\n",
    "pca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desempenho por T2\n",
    "\n",
    "new_status = [0,1]\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix(pca_results['rotulos_bin'], pca_results['T2_classific']), \\\n",
    "                     index=[i for i in new_status], columns=[i for i in new_status])\n",
    "\n",
    "# Linha para normalizar os dados\n",
    "df_cm_norm = round((df_cm.astype('float')/df_cm.sum(axis=1).values.reshape(-1,1)), 3)\n",
    "\n",
    "# Gráfico da matriz de confusão\n",
    "plt.figure(figsize=(6,5), dpi=100)\n",
    "plt.title(\"Confusion Matrix - T2\", fontsize=10)\n",
    "ax = sn.heatmap(df_cm_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"PREDICTED CLASSES\", fontsize=8)\n",
    "ax.set_ylabel(\"REAL CLASSES\", fontsize=8)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(pca_results['rotulos_bin'], pca_results['T2_classific']).ravel()\n",
    "        \n",
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "f1_score = 2*recall*precision/(recall+precision)\n",
    "f2_score = 3*recall*precision/((2*precision)+recall)\n",
    "f05_score = 1.5*recall*precision/((0.5*precision)+recall)\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "specificity = tn/(fp+tn)\n",
    "fpr = fp/(fp+tn)\n",
    "\n",
    "metricas_df = pd.DataFrame([precision, recall, f1_score, f05_score, f2_score, specificity, accuracy, fpr, tp, \\\n",
    "                            tn, fp, fn]).T\n",
    "metricas_df.columns = ['Precision', 'Recall', 'F-score(a=1)', 'F-score(a=0.5)', 'F-score(a=2)', \\\n",
    "                       'Specificity', 'Accuracy', 'FPR(FAR)', 'TP', 'TN', 'FP', 'FN']\n",
    "\n",
    "print(\"\\nOverall Precision:       {:.2f}%\".format((metricas_df['Precision'].values.item()*100)))\n",
    "print(\"Overall Recall:          {:.2f}%\".format((metricas_df['Recall'].values.item()*100)))\n",
    "print(\"Overall F-score(a=1):    {:.2f}%\".format((metricas_df['F-score(a=1)'].values.item()*100)))\n",
    "print(\"Overall F-score(a=0.5):  {:.2f}%\".format((metricas_df['F-score(a=0.5)'].values.item()*100)))\n",
    "print(\"Overall F-score(a=2):    {:.2f}%\".format((metricas_df['F-score(a=2)'].values.item()*100)))\n",
    "print(\"Overall Specificity:     {:.2f}%\".format((metricas_df['Specificity'].values.item()*100)))\n",
    "print(\"Overall FPR(FAR):        {:.2f}%\".format((metricas_df['FPR(FAR)'].values.item()*100)))\n",
    "print(\"Overall Accuracy:        {:.2f}%\".format((metricas_df['Accuracy'].values.item()*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desempenho por Q\n",
    "\n",
    "new_status = [0,1]\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix(pca_results['rotulos_bin'], pca_results['Q_classific']), \\\n",
    "                     index=[i for i in new_status], columns=[i for i in new_status])\n",
    "\n",
    "# Linha para normalizar os dados\n",
    "df_cm_norm = round((df_cm.astype('float')/df_cm.sum(axis=1).values.reshape(-1,1)), 3)\n",
    "\n",
    "# Gráfico da matriz de confusão\n",
    "plt.figure(figsize=(6,5), dpi=100)\n",
    "plt.title(\"Confusion Matrix - Q\", fontsize=10)\n",
    "ax = sn.heatmap(df_cm_norm, annot=True, cmap='PuBu')\n",
    "ax.set_xlabel(\"PREDICTED CLASSES\", fontsize=8)\n",
    "ax.set_ylabel(\"REAL CLASSES\", fontsize=8)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(pca_results['rotulos_bin'], pca_results['Q_classific']).ravel()\n",
    "        \n",
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "f1_score = 2*recall*precision/(recall+precision)\n",
    "f2_score = 3*recall*precision/((2*precision)+recall)\n",
    "f05_score = 1.5*recall*precision/((0.5*precision)+recall)\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "specificity = tn/(fp+tn)\n",
    "fpr = fp/(fp+tn)\n",
    "\n",
    "metricas_df = pd.DataFrame([precision, recall, f1_score, f05_score, f2_score, specificity, accuracy, fpr, tp, \\\n",
    "                            tn, fp, fn]).T\n",
    "metricas_df.columns = ['Precision', 'Recall', 'F-score(a=1)', 'F-score(a=0.5)', 'F-score(a=2)', \\\n",
    "                       'Specificity', 'Accuracy', 'FPR(FAR)', 'TP', 'TN', 'FP', 'FN']\n",
    "\n",
    "print(\"\\nOverall Precision:       {:.2f}%\".format((metricas_df['Precision'].values.item()*100)))\n",
    "print(\"Overall Recall:          {:.2f}%\".format((metricas_df['Recall'].values.item()*100)))\n",
    "print(\"Overall F-score(a=1):    {:.2f}%\".format((metricas_df['F-score(a=1)'].values.item()*100)))\n",
    "print(\"Overall F-score(a=0.5):  {:.2f}%\".format((metricas_df['F-score(a=0.5)'].values.item()*100)))\n",
    "print(\"Overall F-score(a=2):    {:.2f}%\".format((metricas_df['F-score(a=2)'].values.item()*100)))\n",
    "print(\"Overall Specificity:     {:.2f}%\".format((metricas_df['Specificity'].values.item()*100)))\n",
    "print(\"Overall FPR(FAR):        {:.2f}%\".format((metricas_df['FPR(FAR)'].values.item()*100)))\n",
    "print(\"Overall Accuracy:        {:.2f}%\".format((metricas_df['Accuracy'].values.item()*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b7250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
