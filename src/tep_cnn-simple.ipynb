{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, MaxPooling2D, BatchNormalization, Activation, Flatten, GaussianNoise\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support , roc_auc_score, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from pickle import load\n",
    "from timeit import default_timer as timer\n",
    "from random import randint\n",
    "\n",
    "%run ./base_functions.ipynb\n",
    "# %run ./config.ipynb\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixando a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value = randint(0, 99999)\n",
    "print(seed_value)\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos caminhos das pastas\n",
    "\n",
    "data_folder = \"D:\\\\TEP - Python\\\\\"\n",
    "outputs_folder = \"C:\\\\Users\\\\anaso\\\\Desktop\\\\workspace\\\\doutorado\\\\outputs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = 'T1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_stride_ana(array, max_time, sub_window_size, stride_size):\n",
    "    \n",
    "    sub_windows = ( \n",
    "        np.expand_dims(np.arange(sub_window_size), 0) +\n",
    "        np.expand_dims(np.arange(max_time + 1), 0).T\n",
    "    )\n",
    "    \n",
    "    # Descobre o index da última coluna do array\n",
    "    last_col_index = (array.shape[1])-1\n",
    "    \n",
    "    # Linha da matriz de índices que vai até o tamanho total do trecho que será convertido em matrizes\n",
    "    cut_point = np.where(sub_windows[:,last_col_index] == len(array)-1)[0].item()\n",
    "    \n",
    "    # Faz o corte\n",
    "    sub_windows_new = sub_windows[:cut_point+1] # adicionei o +1 pra bater com o número total de matreizes\n",
    "    \n",
    "    # Fancy indexing to select every V rows.\n",
    "    return array[sub_windows_new[::stride_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_folder + \"09-python_dados-3anos.csv\", sep=';')\n",
    "data_total = data.drop(['Unnamed: 0', 'STATUS'], 1)\n",
    "\n",
    "data2 = pd.read_csv(data_folder + \"13-python_dados-1ano.csv\", sep=';')\n",
    "data_teste = data2.drop(['Unnamed: 0', 'STATUS'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Treino: \", np.shape(data_total))\n",
    "print(\"Teste:  \", np.shape(data_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_status = pd.read_csv(data_folder + \"09-params-3anos.csv\", sep=';')\n",
    "sim_status = sim_status.drop(['Unnamed: 0'], 1)\n",
    "\n",
    "sim_status2 = pd.read_csv(data_folder + \"13-params-1ano.csv\", sep=';')\n",
    "sim_status_teste = sim_status2.drop(['Unnamed: 0'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlinhas = 52\n",
    "ncolunas = 52\n",
    "sliding_window = 5\n",
    "\n",
    "ti = timer()\n",
    "\n",
    "x_windows, y_windows, y_windows_ohe = matrix_generator(data_total, sim_status, nlinhas, ncolunas, sliding_window)\n",
    "\n",
    "tf = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tempo total: \" + str(int((tf-ti)//60)) + \" minutos e \" + str(math.ceil((tf-ti)%60))+ \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão randômica e estratificada em treino e validação\n",
    "\n",
    "X_train, X_valid, y_train_multi, y_valid_multi = train_test_split(x_windows, y_windows, \n",
    "                                                                  test_size=0.15, \n",
    "                                                                  random_state=seed_value, \n",
    "                                                                  shuffle=True, \n",
    "                                                                  stratify=y_windows)\n",
    "\n",
    "print(\"\\nTREINO\")\n",
    "print(\"X: \", np.shape(X_train))\n",
    "print(\"Y: \", np.shape(y_train_multi))\n",
    "print(\"Status:\", Counter(y_train_multi))\n",
    "\n",
    "print(\"\\nVALIDAÇÃO\")\n",
    "print(\"X: \", np.shape(X_valid))\n",
    "print(\"Y: \", np.shape(y_valid_multi))\n",
    "print(\"Status:\", Counter(y_valid_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS = np.unique(y_train_multi)\n",
    "\n",
    "# Faz um novo OHE\n",
    "y_windows_train_ohe = to_categorical(y_train_multi, num_classes=len(STATUS))\n",
    "y_windows_valid_ohe = to_categorical(y_valid_multi, num_classes=len(STATUS))\n",
    "\n",
    "y_train = y_windows_train_ohe\n",
    "y_valid = y_windows_valid_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "df_train = pd.DataFrame(X_train.reshape((nlinhas*X_train.shape[0], nlinhas)))\n",
    "df_valid = pd.DataFrame(X_valid.reshape((nlinhas*X_valid.shape[0], nlinhas)))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train)\n",
    "\n",
    "df_train_norm = scaler.transform(df_train)\n",
    "df_valid_norm = scaler.transform(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_to_4d_train = vectorized_stride_ana(df_train_norm, len(df_train_norm)-1, nlinhas, nlinhas)\n",
    "x_train = back_to_4d_train.reshape((len(back_to_4d_train), nlinhas, ncolunas, 1), order='C')\n",
    "\n",
    "back_to_4d_valid = vectorized_stride_ana(df_valid_norm, len(df_valid_norm)-1, nlinhas, nlinhas)\n",
    "x_valid = back_to_4d_valid.reshape((len(back_to_4d_valid), nlinhas, ncolunas, 1), order='C')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(x_train.shape)\n",
    "print()\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste extra sem shuffle\n",
    "\n",
    "# Scaling de acordo com o treino\n",
    "extra_test_norm = pd.DataFrame(scaler.transform(data_teste), columns=data_teste.columns)\n",
    "\n",
    "# Geração das matrizes\n",
    "x_test_extra, y_windows, y_windows_ohe = matrix_generator(extra_test_norm, sim_status_teste, nlinhas, ncolunas, \\\n",
    "                                                          sliding_window)\n",
    "\n",
    "print()\n",
    "print(x_test_extra.shape)\n",
    "print(Counter(y_windows))\n",
    "\n",
    "y_test_extra = to_categorical(y_windows, num_classes=len(STATUS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test_extra.shape)\n",
    "print(y_test_extra.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem do sistema de FDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de class_weight para o caso multilabel\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', \n",
    "                                                  classes=np.unique(y_train.argmax(axis=1)), \n",
    "                                                  y=y_train.argmax(axis=1))\n",
    "\n",
    "class_weight_dict = {}\n",
    "for i in range(len(STATUS)):\n",
    "        class_weight_dict[i] = class_weights[i]\n",
    "\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo - Treino simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definição da topologia do modelo\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=20, \n",
    "                 kernel_size=(3,3),\n",
    "                 strides=1,\n",
    "                 padding=\"same\",\n",
    "                 data_format='channels_last', \n",
    "                 use_bias=True,\n",
    "                 input_shape=(nlinhas,ncolunas,1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(filters=30, \n",
    "                 kernel_size=(3,3),\n",
    "                 strides=1,\n",
    "                 padding=\"same\",\n",
    "                 kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                 bias_regularizer=regularizers.l2(l2_reg),\n",
    "                 activity_regularizer=regularizers.l2(l2_reg),\n",
    "                 use_bias=True))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2, data_format='channels_last'))\n",
    "\n",
    "model.add(Conv2D(filters=50, \n",
    "                 kernel_size=(3,3),\n",
    "                 strides=1,\n",
    "                 padding=\"same\", \n",
    "                 use_bias=True))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2, data_format='channels_last'))\n",
    "\n",
    "model.add(Flatten()) \n",
    "\n",
    "model.add(Dense(units=(len(STATUS)), activation='softmax', use_bias=True))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Treinamento da rede convolucional\n",
    "\n",
    "# Definição dos callbacks usados\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='auto', patience=20, restore_best_weights=True)\n",
    "model_check = ModelCheckpoint(filepath=outputs_folder+\"model_cp_\"+TEST_CODE+\".h5\", monitor=\"val_loss\", mode=\"auto\")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), \n",
    "              metrics=['acc', Precision(), Recall()])\n",
    "\n",
    "ti = timer()\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=500, \n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    verbose=1,\n",
    "                    shuffle=False,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    callbacks=[early_stopping, model_check]) \n",
    "\n",
    "save_model(model=model, iterator=TEST_CODE, train_type='simple', model_name='CNN')\n",
    "\n",
    "tf = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tempo total: \" + str(int((tf-ti)//60)) + \" minutos e \" + str(math.ceil((tf-ti)%60))+ \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gráficos - Treinamento x Validação\n",
    "\n",
    "# Informações do treinamento\n",
    "try:\n",
    "    train_acc = history.history[model.metrics_names[1]]\n",
    "    train_loss = history.history[model.metrics_names[0]]\n",
    "    train_precision = history.history[model.metrics_names[2]]\n",
    "    train_recall = history.history[model.metrics_names[3]]\n",
    "except:\n",
    "    # quando o history vem do CSV salvo do melhor modelo\n",
    "    train_acc = history['acc']\n",
    "    train_loss = history['loss']\n",
    "    train_precision = history['precision_'+str(best_trial)]\n",
    "    train_recall = history['recall_'+str(best_trial)]\n",
    "\n",
    "# Informações da validação\n",
    "try:\n",
    "    val_acc = history.history['val_'+str(model.metrics_names[1])]\n",
    "    val_loss = history.history['val_'+str(model.metrics_names[0])]\n",
    "    val_precision = history.history['val_'+str(model.metrics_names[2])]\n",
    "    val_recall = history.history['val_'+str(model.metrics_names[3])]\n",
    "except:\n",
    "    val_acc = history['val_acc']\n",
    "    val_loss = history['val_loss']\n",
    "    val_precision = history['val_precision_'+str(best_trial)]\n",
    "    val_recall = history['val_recall_'+str(best_trial)]\n",
    "\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_acc, '-bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, '-ko', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_loss, '-bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, '-ko', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_precision, '-bo', label='Training precision')\n",
    "plt.plot(epochs, val_precision, '-ko', label='Validation precision')\n",
    "plt.title('Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, train_recall, '-bo', label='Training recall')\n",
    "plt.plot(epochs, val_recall, '-ko', label='Validation recall')\n",
    "plt.title('Recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "\n",
    "train_metrics = display_metrics(x_train, y_train, model, STATUS, 'Treino', multi_problem=True) \n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validação\n",
    "\n",
    "valid_metrics = display_metrics(x_valid, y_valid, model, STATUS, 'Validação', multi_problem=True) \n",
    "valid_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Teste\n",
    "\n",
    "test_metrics = display_metrics(x_test, y_test, model, STATUS, 'Teste', multi_problem=True) \n",
    "test_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
