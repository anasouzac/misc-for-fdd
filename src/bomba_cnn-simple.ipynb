{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import interp\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, MaxPooling2D, BatchNormalization, Activation, Flatten, GaussianNoise\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support , roc_auc_score, auc, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from pickle import load\n",
    "from timeit import default_timer as timer\n",
    "from random import randint\n",
    "from itertools import cycle\n",
    "\n",
    "%run ./base_functions.ipynb\n",
    "# %run ./config.ipynb\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixando a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value = randint(0, 99999)\n",
    "print(seed_value)\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição dos caminhos das pastas\n",
    "\n",
    "data_folder = \"D:\\\\Caso 2 - Bomba Centrífuga\\\\Bancos tratados\\\\\"\n",
    "outputs_folder = \"C:\\\\Users\\\\anaso\\\\Desktop\\\\workspace\\\\doutorado\\\\outputs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CODE = 'T1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_stride_ana(array, max_time, sub_window_size, stride_size):\n",
    "    \n",
    "    sub_windows = ( \n",
    "        np.expand_dims(np.arange(sub_window_size), 0) +\n",
    "        np.expand_dims(np.arange(max_time + 1), 0).T\n",
    "    )\n",
    "    \n",
    "    # Descobre o index da última coluna do array\n",
    "    last_col_index = (array.shape[1])-1\n",
    "    \n",
    "    # Linha da matriz de índices que vai até o tamanho total do trecho que será convertido em matrizes\n",
    "    cut_point = np.where(sub_windows[:,last_col_index] == len(array)-1)[0].item()\n",
    "    \n",
    "    # Faz o corte\n",
    "    sub_windows_new = sub_windows[:cut_point+1] # adicionei o +1 pra bater com o número total de matreizes\n",
    "    \n",
    "    # Fancy indexing to select every V rows.\n",
    "    return array[sub_windows_new[::stride_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_folder + \"banco_labeling-v1.csv\", sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data = data[['Time']]\n",
    "x_data = data.drop(['rotulos_multi', 'rotulos_bin', 'Time'], axis=1)\n",
    "y_bin = data[['rotulos_bin']]\n",
    "y_multi = data[['rotulos_multi']]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_multi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_multi['rotulos_multi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_bin['rotulos_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(data_folder + \"banco_labeling_params-v1.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Auxilia a identificar o ponto no qual o banco precisa ser dividido\n",
    "params[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove colunas desnecessárias\n",
    "params.drop(['status_init', 'status_end'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params = params.iloc[:74,:]\n",
    "new_x_data = x_data.loc[:1212560,:]\n",
    "new_y_multi = y_multi.loc[:1212560]\n",
    "\n",
    "x_test = x_data.loc[1212560:,:]\n",
    "test_params = params[74:]\n",
    "y_test = y_multi.loc[1212560:]\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(Counter(y_test['rotulos_multi']))\n",
    "print()\n",
    "\n",
    "print(x_data.shape)\n",
    "print(\"\", new_x_data.shape[0] + x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlinhas = 25\n",
    "ncolunas = 25\n",
    "sliding_window = 5\n",
    "\n",
    "ti = timer()\n",
    "x_windows, y_windows, y_windows_ohe = matrix_generator(new_x_data, new_params, nlinhas, ncolunas, \\\n",
    "                                                       sliding_window)\n",
    "tf = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tempo total: \" + str(int((tf-ti)//60)) + \" minutos e \" + str(math.ceil((tf-ti)%60))+ \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão randômica e estratificada em treino e validação\n",
    "\n",
    "X_train, X_valid, y_train_multi, y_valid_multi = train_test_split(x_windows, y_windows, \n",
    "                                                                  test_size=0.15, \n",
    "                                                                  random_state=seed_value, \n",
    "                                                                  shuffle=True, \n",
    "                                                                  stratify=y_windows)\n",
    "\n",
    "print(\"\\nTREINO\")\n",
    "print(\"X: \", np.shape(X_train))\n",
    "print(\"Y: \", np.shape(y_train_multi))\n",
    "print(\"Status:\", Counter(y_train_multi))\n",
    "\n",
    "print(\"\\nVALIDAÇÃO\")\n",
    "print(\"X: \", np.shape(X_valid))\n",
    "print(\"Y: \", np.shape(y_valid_multi))\n",
    "print(\"Status:\", Counter(y_valid_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os rótulos em binário\n",
    "y_windows_train_bin = np.where(y_train_multi != 0, 1, y_train_multi)\n",
    "y_windows_valid_bin = np.where(y_valid_multi != 0, 1, y_valid_multi)\n",
    "\n",
    "# Faz um novo OHE\n",
    "y_windows_train_ohe_bin = to_categorical(y_windows_train_bin, num_classes=2)\n",
    "y_windows_valid_ohe_bin = to_categorical(y_windows_valid_bin, num_classes=2)\n",
    "\n",
    "y_train = y_windows_train_ohe_bin\n",
    "y_valid = y_windows_valid_ohe_bin\n",
    "\n",
    "STATUS = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "df_train = pd.DataFrame(X_train.reshape((25*X_train.shape[0], 25)))\n",
    "df_valid = pd.DataFrame(X_valid.reshape((25*X_valid.shape[0], 25)))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train)\n",
    "\n",
    "df_train_norm = scaler.transform(df_train)\n",
    "df_valid_norm = scaler.transform(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_to_4d_train = vectorized_stride_ana(df_train_norm, len(df_train_norm)-1, nlinhas, nlinhas)\n",
    "x_train = back_to_4d_train.reshape((len(back_to_4d_train), nlinhas, ncolunas, 1), order='C')\n",
    "\n",
    "back_to_4d_valid = vectorized_stride_ana(df_valid_norm, len(df_valid_norm)-1, nlinhas, nlinhas)\n",
    "x_valid = back_to_4d_valid.reshape((len(back_to_4d_valid), nlinhas, ncolunas, 1), order='C')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(x_train.shape)\n",
    "print()\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste sem shuffle\n",
    "\n",
    "test_norm = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
    "x_test, y_windows, y_windows_ohe = matrix_generator(test_norm, test_params, nlinhas, ncolunas, \\\n",
    "                                                    sliding_window)\n",
    "print()\n",
    "print(x_test.shape)\n",
    "print(y_windows.shape)\n",
    "print(Counter(y_windows))\n",
    "\n",
    "y_windows_test_bin = np.where(y_windows != 0, 1, y_windows)\n",
    "y_test = to_categorical(y_windows_test_bin, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem do sistema de FDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(y_train.argmax(axis=1)) \n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight_dict = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo - Treino simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definição da topologia do modelo\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=10,\n",
    "        kernel_size=(3,3),\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        data_format='channels_last',\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        activation='relu',\n",
    "        input_shape=(nlinhas,ncolunas,1)\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=20,\n",
    "        kernel_size=(3,3),\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        data_format='channels_last',\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        activation='relu',\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2, data_format='channels_last'))\n",
    "\n",
    "# model.add(\n",
    "#     Conv2D(\n",
    "#         filters=30,\n",
    "#         kernel_size=(3,3),\n",
    "#         strides=1,\n",
    "#         padding=\"same\",\n",
    "#         data_format='channels_last',\n",
    "#         use_bias=True,\n",
    "#         kernel_initializer=\"glorot_uniform\",\n",
    "#         bias_initializer=\"zeros\",\n",
    "#         activation='relu',\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(2,2), strides=2, data_format='channels_last'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=len(STATUS), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Treinamento da rede convolucional\n",
    "\n",
    "# Definição dos callbacks usados\n",
    "callbacks_list = [EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "                  ModelCheckpoint(filepath=outputs_folder+\"model_cp_\"+TEST_CODE+\".h5\", monitor=\"val_loss\", mode=\"auto\")]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), \n",
    "              metrics=['acc', Precision(), Recall()])\n",
    "\n",
    "ti = timer()\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=10,#0, \n",
    "                    batch_size=500, \n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    verbose=1,\n",
    "                    shuffle=False,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    callbacks=callbacks_list) \n",
    "\n",
    "save_model(model=model, iterator=TEST_CODE, train_type='simple', model_name='CNN')\n",
    "\n",
    "tf = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tempo total: \" + str(int((tf-ti)//60)) + \" minutos e \" + str(math.ceil((tf-ti)%60))+ \" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gráficos - Treinamento x Validação\n",
    "\n",
    "# Informações do treinamento\n",
    "try:\n",
    "    train_acc = history.history[model.metrics_names[1]]\n",
    "    train_loss = history.history[model.metrics_names[0]]\n",
    "    train_precision = history.history[model.metrics_names[2]]\n",
    "    train_recall = history.history[model.metrics_names[3]]\n",
    "except:\n",
    "    # quando o history vem do CSV salvo do melhor modelo\n",
    "    train_acc = history['acc']\n",
    "    train_loss = history['loss']\n",
    "    train_precision = history['precision_'+str(best_trial)]\n",
    "    train_recall = history['recall_'+str(best_trial)]\n",
    "\n",
    "# Informações da validação\n",
    "try:\n",
    "    val_acc = history.history['val_'+str(model.metrics_names[1])]\n",
    "    val_loss = history.history['val_'+str(model.metrics_names[0])]\n",
    "    val_precision = history.history['val_'+str(model.metrics_names[2])]\n",
    "    val_recall = history.history['val_'+str(model.metrics_names[3])]\n",
    "except:\n",
    "    val_acc = history['val_acc']\n",
    "    val_loss = history['val_loss']\n",
    "    val_precision = history['val_precision_'+str(best_trial)]\n",
    "    val_recall = history['val_recall_'+str(best_trial)]\n",
    "\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "print(\"Épocas: \", len(epochs))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_acc, '-bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, '-ko', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_loss, '-bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, '-ko', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_precision, '-bo', label='Training precision')\n",
    "plt.plot(epochs, val_precision, '-ko', label='Validation precision')\n",
    "plt.title('Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, train_recall, '-bo', label='Training recall')\n",
    "plt.plot(epochs, val_recall, '-ko', label='Validation recall')\n",
    "plt.title('Recall')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "\n",
    "train_metrics = display_metrics(x_train, y_train, model, STATUS, 'Treino', multi_problem=False) \n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Validação\n",
    "\n",
    "valid_metrics = display_metrics(x_valid, y_valid, model, STATUS, 'Validação', multi_problem=False) \n",
    "valid_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Teste\n",
    "\n",
    "test_metrics = display_metrics(x_test, y_test, model, STATUS, 'Teste', multi_problem=False) \n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "\n",
    "plt.figure(figsize=(12,3), dpi=100)\n",
    "plt.plot(y_test.argmax(axis=1))\n",
    "plt.title(\"Real\")\n",
    "\n",
    "plt.figure(figsize=(12,3), dpi=100)\n",
    "plt.plot(pred.argmax(axis=1))\n",
    "plt.title(\"Predito\")\n",
    "\n",
    "plt.figure(figsize=(12,3), dpi=100)\n",
    "plt.plot(x_test['VT-322'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_roc_auc(x_test, y_test, model, 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construção da curva ROC para o caso binário (base: classe 0)\n",
    "\n",
    "model_pred = model.predict(x_test)\n",
    "\n",
    "# Predições em OHE\n",
    "y_pred = pd.DataFrame(model_pred.argmax(axis=1)).astype('category')\n",
    "y_pred = pd.get_dummies(y_pred).values\n",
    "\n",
    "n_classes = len(STATUS)\n",
    "\n",
    "# Calcula a curva ROC e a métrica AUC para cada classe\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i], )\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure(figsize=(5,5), dpi=300)\n",
    "plt.plot(fpr[0], tpr[0], color='darkorange', lw=2, label='Curva ROC (area = %0.3f)' % roc_auc[0])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de falsos positivos')\n",
    "plt.ylabel('Taxa de verdadeiros positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC e cálculo da métrica AUC para todas as classes\n",
    "\n",
    "roc_auc_scores = []\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12,12), dpi=300)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.3f})'''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:0.3f})'''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.3f})'''.format(i, roc_auc[i]))\n",
    "    roc_auc_scores.append(roc_auc[i])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de falsos positivos')\n",
    "plt.ylabel('Taxa de verdadeiros positivos')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
